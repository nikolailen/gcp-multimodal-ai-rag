{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hABzz5KJsFn1",
   "metadata": {
    "id": "hABzz5KJsFn1"
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g06wg_WnsVA0",
   "metadata": {
    "id": "g06wg_WnsVA0"
   },
   "source": [
    "## Pip Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0QdN_4mCItL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92431,
     "status": "ok",
     "timestamp": 1739979627709,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "a0QdN_4mCItL",
    "outputId": "e7958df5-c7d4-4e19-db61-1ae1cfc2791e"
   },
   "outputs": [],
   "source": [
    "# Force pip to re-check versions so old conflicting libraries are removed:\n",
    "!pip install --quiet --force-reinstall \\\n",
    "  \"pydantic==2.10.4\" \\\n",
    "  \"google-cloud-aiplatform==1.78.0\" \\\n",
    "  \"langchain-google-vertexai==2.0.13\" \\\n",
    "  \"langchain-google-community[bigquery]\" \\\n",
    "  \"pymupdf\" \\\n",
    "  \"google-cloud-secret-manager\" \\\n",
    "  \"google-api-python-client\" \\\n",
    "  \"google-auth\" \\\n",
    "  \"google-auth-httplib2\" \\\n",
    "  \"google-auth-oauthlib\" \\\n",
    "  \"urllib3[secure]\" \\\n",
    "  \"requests[secure]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pq-b6qGHqvIk",
   "metadata": {
    "id": "Pq-b6qGHqvIk"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tiPMSy4lmiP3",
   "metadata": {
    "executionInfo": {
     "elapsed": 8999,
     "status": "ok",
     "timestamp": 1739979641375,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "tiPMSy4lmiP3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import pytz\n",
    "import hashlib\n",
    "import logging\n",
    "import pymupdf  # Keep pymupdf in imports\n",
    "import pandas as pd\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from collections import OrderedDict\n",
    "from io import StringIO\n",
    "from zoneinfo import ZoneInfo\n",
    "from urllib.parse import quote, urlparse  # Import for URL encoding and URI parsing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "import google.auth.transport.requests\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from googleapiclient.errors import HttpError\n",
    "from google.cloud import storage, bigquery, secretmanager\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_community import BigQueryVectorStore, BigQueryLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fOolRDvpsq8l",
   "metadata": {
    "id": "fOolRDvpsq8l"
   },
   "source": [
    "# Copying New Files to Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XmCnpR4rmogI",
   "metadata": {
    "id": "XmCnpR4rmogI"
   },
   "source": [
    "## Retrieve current contents of the Drive folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jPM9iyy1tNWd",
   "metadata": {
    "id": "jPM9iyy1tNWd"
   },
   "source": [
    "To add new Drive folders, please share the folder with the service account `your-project-id-gcs-drive@your-project-id.iam.gserviceaccount.com`, granting it view access. Additionally, include the unique folder ID in the `folder_ids` list using the following format: `'1ABC2DEF3GHI4JKL5MNO6PQR7STU8VWX'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Qp3_muruz2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329207,
     "status": "ok",
     "timestamp": 1739982505869,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "7Qp3_muruz2d",
    "outputId": "84abf48e-1cb5-48fe-9b59-d209887ec5ad"
   },
   "outputs": [],
   "source": [
    "# Initialize Secret Manager Client\n",
    "secret_client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "# Get default credentials and the project number\n",
    "_, project_number = google.auth.default()\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    secret_path = f\"projects/{project_number}/secrets/{secret_name}/versions/latest\"\n",
    "    response = secret_client.access_secret_version(name=secret_path)\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "# Fetch the service account JSON string from Secret Manager\n",
    "service_account_json = get_secret(\"your-drive-service-account-json\")\n",
    "\n",
    "# Parse the JSON string to a Python dictionary\n",
    "service_account_info = json.loads(service_account_json)\n",
    "\n",
    "# Define the scopes required for the APIs\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/drive.readonly',\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/devstorage.full_control',\n",
    "    'https://www.googleapis.com/auth/bigquery'\n",
    "]\n",
    "\n",
    "# Create service account credentials\n",
    "credentials = service_account.Credentials.from_service_account_info(\n",
    "    service_account_info, scopes=SCOPES\n",
    ")\n",
    "\n",
    "# Build the Drive service\n",
    "drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "def robust_files_list(service, query, max_retries=5, initial_wait=1):\n",
    "    \"\"\"\n",
    "    Perform a Drive `files().list()` request with exponential backoff\n",
    "    for handling transient server errors.\n",
    "    \"\"\"\n",
    "    # You can add whichever status codes you consider 'transient' here.\n",
    "    transient_errors = [429, 500, 502, 503, 504]\n",
    "\n",
    "    attempt = 0\n",
    "    wait_time = initial_wait\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return service.files().list(\n",
    "                q=query,\n",
    "                fields='files(id, name, mimeType, createdTime, parents, webViewLink, size)',\n",
    "                includeItemsFromAllDrives=True,  # <--- Added here\n",
    "                supportsAllDrives=True          # <--- And here\n",
    "            ).execute()\n",
    "        except HttpError as e:\n",
    "            status = e.resp.status\n",
    "            if status in transient_errors and attempt < max_retries:\n",
    "                print(f\"Server error ({status}). Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                wait_time *= 2  # Exponential backoff\n",
    "                attempt += 1\n",
    "            else:\n",
    "                # Reraise if it's not in our transient list\n",
    "                # or we've exceeded the retry limit.\n",
    "                raise\n",
    "\n",
    "\n",
    "def traverse_folders(service, folder_id, all_files, parent_path=None, root_folder_name=None):\n",
    "    \"\"\"\n",
    "    Traverse all folders and files, collecting details.\n",
    "    \"\"\"\n",
    "    # Initialize parent path with the root folder name if not provided\n",
    "    if parent_path is None:\n",
    "        parent_path = root_folder_name\n",
    "\n",
    "    query = f\"'{folder_id}' in parents\"\n",
    "    response = robust_files_list(service, query)\n",
    "\n",
    "    for item in response.get('files', []):\n",
    "        file_name = item['name']\n",
    "        mime_type = item.get('mimeType')\n",
    "        created_time = item.get('createdTime', 'N/A')\n",
    "        parent_folder_id = item.get('parents', ['N/A'])[0]\n",
    "        web_view_link = item.get('webViewLink', 'N/A')\n",
    "        file_size = float(item.get('size', 0)) / (1024 * 1024)  # Convert size to MB\n",
    "\n",
    "        # Build the full path (folder hierarchy)\n",
    "        full_path = f\"{parent_path}/{file_name}\"\n",
    "        all_files.append({\n",
    "            'file_id': item['id'],\n",
    "            'file_name': file_name,\n",
    "            'file_created_time': created_time,\n",
    "            'parent_folder_id': parent_folder_id,\n",
    "            'web_view_link': web_view_link,\n",
    "            'file_path': full_path,\n",
    "            'file_mime_type': mime_type,\n",
    "            'file_size': file_size,  # File size in MB\n",
    "            'copied_to_gcs': False,\n",
    "            'parsed': 'false'\n",
    "        })\n",
    "\n",
    "        # Recurse into subfolders\n",
    "        if mime_type == 'application/vnd.google-apps.folder':\n",
    "            new_parent_path = f\"{parent_path}/{file_name}\"\n",
    "            traverse_folders(service, item['id'], all_files, new_parent_path, root_folder_name)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# LIST OF FOLDER IDS\n",
    "# Add or remove folder IDs here as needed\n",
    "folder_ids = [\n",
    "    '1ABC2DEF3GHI4JKL5MNO6PQR7STU8VWX',    # Folder 1\n",
    "    #'2DEF4GHI5JKL6MNO7PQR8STU9VWX0ABC1',  # Folder 2\n",
    "    #'3EFG5HIJ6KLM7NOP8QRS9TUV0WXY1BCD2',  # Folder 3\n",
    "    #'4FGH6IJK7LMN8OPQ9RST0UVW1XYZ2CDE3',  # Folder 4\n",
    "    #'5GHI7JKL8MNO9PQR0STU1VWX2YZA3DEF4',  # Folder 5\n",
    "]\n",
    "\n",
    "successful_folders = 0\n",
    "\n",
    "# Create a list to hold details for all files from all folders\n",
    "master_files = []\n",
    "\n",
    "# Process each folder ID in the list\n",
    "for idx, folder_id in enumerate(folder_ids, start=1):\n",
    "    try:  # <-- Added\n",
    "        # Get the root folder name for this folder ID\n",
    "        root_folder = drive_service.files().get(\n",
    "            fileId=folder_id,\n",
    "            fields='name',\n",
    "            supportsAllDrives=True\n",
    "        ).execute()\n",
    "        root_folder_name = root_folder.get('name', f\"Unknown_{idx}\")\n",
    "\n",
    "        # Print a statement showing which folder we're about to process\n",
    "        print(f\"\\nProcessing folder #{idx}: {root_folder_name} (ID: {folder_id})\")\n",
    "\n",
    "        # Create a fresh list *just* for this folder\n",
    "        folder_files = []\n",
    "\n",
    "        # Traverse the Google Drive structure for this folder\n",
    "        traverse_folders(\n",
    "            drive_service,\n",
    "            folder_id,\n",
    "            folder_files,\n",
    "            parent_path=None,\n",
    "            root_folder_name=root_folder_name\n",
    "        )\n",
    "\n",
    "        # -----------------\n",
    "        # FOLDER-LEVEL ANALYTICS\n",
    "        # -----------------\n",
    "        total_files = len(folder_files)\n",
    "        total_size_gb = sum(f['file_size'] for f in folder_files) / 1024  # MB -> GB\n",
    "        pdf_size_gb = sum(\n",
    "            f['file_size']\n",
    "            for f in folder_files\n",
    "            if f['file_mime_type'] == 'application/pdf'\n",
    "        ) / 1024\n",
    "\n",
    "        # Count files by mime type\n",
    "        mime_type_counts = {}\n",
    "        for f in folder_files:\n",
    "            mt = f['file_mime_type']\n",
    "            mime_type_counts[mt] = mime_type_counts.get(mt, 0) + 1\n",
    "\n",
    "        # Print analytics for this folder\n",
    "        print(f\"\\n=== Folder #{idx}: {root_folder_name} ===\")\n",
    "        print(f\"Total number of files: {total_files}\")\n",
    "        print(f\"Total size of all files: {total_size_gb:.2f} GB\")\n",
    "        print(f\"Total size of PDF files: {pdf_size_gb:.2f} GB\")\n",
    "        print(\"Number of files by mime type:\")\n",
    "        for mt, count in mime_type_counts.items():\n",
    "            print(f\" - {mt}: {count}\")\n",
    "\n",
    "        # Save this folder's file details to CSV\n",
    "        df_folder = pd.DataFrame(folder_files)\n",
    "        output_filename = f\"drive_files_report_{root_folder_name}.csv\"\n",
    "        df_folder.to_csv(output_filename, index=False)\n",
    "        print(f\"File details for this folder saved to '{output_filename}'\")\n",
    "\n",
    "        #If we get here, it means this folder was processed successfully\n",
    "        successful_folders += 1\n",
    "\n",
    "        # -----------------\n",
    "        # MASTER LIST\n",
    "        # -----------------\n",
    "        # Append the folder_files to the master_files\n",
    "        master_files.extend(folder_files)\n",
    "\n",
    "    except HttpError as e:\n",
    "        # 3) Skip this folder on error but don't crash\n",
    "        print(f\"Skipping folder #{idx} (ID: {folder_id}) due to error: {e}\")\n",
    "\n",
    "# After the for loop ends\n",
    "if successful_folders == 0:\n",
    "    raise RuntimeError(\"All folder IDs were unavailable or produced errors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0uvJPCzmd-4",
   "metadata": {
    "id": "e0uvJPCzmd-4"
   },
   "source": [
    "## Update BQ file register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zjD2iba-vA0o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4271,
     "status": "ok",
     "timestamp": 1739982522235,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "zjD2iba-vA0o",
    "outputId": "639e8106-a7a9-415b-a678-5b43c86b8992"
   },
   "outputs": [],
   "source": [
    "# Initialize the BigQuery client\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "# BigQuery table details\n",
    "table_id = get_secret(\"YOUR_TABLE_ID\")  # Replace with your BigQuery table name\n",
    "\n",
    "# Retrieve existing file IDs from BigQuery\n",
    "query = f\"SELECT file_id, file_number FROM `{table_id}`\"\n",
    "query_job = bq_client.query(query)\n",
    "results = list(query_job)\n",
    "\n",
    "# Extract existing file IDs and the maximum file_number\n",
    "existing_file_ids = {row['file_id'] for row in results}\n",
    "existing_file_numbers = [row['file_number'] for row in results]\n",
    "\n",
    "# Determine the starting file_number for new files\n",
    "next_file_number = max(existing_file_numbers, default=0) + 1\n",
    "\n",
    "# Filter new files (those not already in BigQuery)\n",
    "new_files = [file for file in master_files if file['file_id'] not in existing_file_ids]\n",
    "\n",
    "if new_files:\n",
    "    print(f\"Number of new files to add to BigQuery: {len(new_files)}\")\n",
    "    # Prepare the data for BigQuery insertion\n",
    "    new_df = pd.DataFrame(new_files)\n",
    "\n",
    "    # Convert 'file_created_time' to datetime\n",
    "    new_df['file_created_time'] = pd.to_datetime(new_df['file_created_time'], errors='coerce')\n",
    "\n",
    "    # Assign file_number sequentially starting from next_file_number\n",
    "    new_df['file_number'] = range(next_file_number, next_file_number + len(new_df))\n",
    "\n",
    "    # Add the current timestamp for when the record is added\n",
    "    paris_tz = ZoneInfo(\"Europe/Paris\")\n",
    "    paris_time = datetime.now(paris_tz)\n",
    "    new_df['added_to_list'] = paris_time.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "\n",
    "    # Upload new data to BigQuery\n",
    "    job = bq_client.load_table_from_dataframe(new_df, table_id)\n",
    "    job.result()  # Wait for the job to complete\n",
    "    print(f\"Added {len(new_files)} new files to BigQuery table '{table_id}'.\")\n",
    "else:\n",
    "    print(\"No new files to add to BigQuery.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B9hHKCD5oJKD",
   "metadata": {
    "id": "B9hHKCD5oJKD"
   },
   "source": [
    "\n",
    "## Copy files to GCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jxfLtNcNqDbC",
   "metadata": {
    "id": "jxfLtNcNqDbC"
   },
   "outputs": [],
   "source": [
    "batch_size = None  # Set the batch size\n",
    "\n",
    "# Enable detailed logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "secret_client = secretmanager.SecretManagerServiceClient()\n",
    "_, project_number = google.auth.default()\n",
    "credentials, project_id = google.auth.default()\n",
    "\n",
    "# Lock for BigQuery updates\n",
    "bq_lock = threading.Lock()\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    secret_path = f\"projects/{project_number}/secrets/{secret_name}/versions/latest\"\n",
    "    response = secret_client.access_secret_version(name=secret_path)\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "# Fetch the service account JSON string from Secret Manager\n",
    "service_account_json = get_secret(\"your-drive-service-account-json\")\n",
    "\n",
    "# Parse the JSON string to a Python dictionary\n",
    "service_account_info = json.loads(service_account_json)\n",
    "\n",
    "# Scopes required for the APIs\n",
    "SCOPES = [\n",
    "    'https://www.googleapis.com/auth/drive',\n",
    "    'https://www.googleapis.com/auth/cloud-platform',\n",
    "    'https://www.googleapis.com/auth/devstorage.full_control',\n",
    "    'https://www.googleapis.com/auth/bigquery'\n",
    "]\n",
    "\n",
    "# Authenticate with the credentials using the service account info\n",
    "credentials = service_account.Credentials.from_service_account_info(\n",
    "    service_account_info, scopes=SCOPES)\n",
    "\n",
    "# Initialize the Cloud Storage client\n",
    "storage_client = storage.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Initialize the BigQuery client\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "# Set Paris timezone\n",
    "paris_tz = ZoneInfo('Europe/Paris')\n",
    "\n",
    "# GCS bucket and folder information\n",
    "bucket_name = get_secret(\"YOUR_GCS_BUCKET\")\n",
    "table_id = get_secret(\"YOUR_TABLE_ID\")\n",
    "processed_folder = get_secret(\"YOUR_PROCESSED_FOLDER\")\n",
    "\n",
    "# Initialize the GCS bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "# Thread-local storage for drive_service\n",
    "thread_local = threading.local()\n",
    "\n",
    "def get_drive_service():\n",
    "    if not hasattr(thread_local, 'drive_service'):\n",
    "        # Build the Drive service for this thread\n",
    "        thread_local.drive_service = build('drive', 'v3', credentials=credentials)\n",
    "    return thread_local.drive_service\n",
    "\n",
    "# Function to process each file\n",
    "def process_file(file):\n",
    "    try:\n",
    "        # Get the thread-local drive service\n",
    "        drive_service = get_drive_service()\n",
    "\n",
    "        # Extract file_id, file_name, and file_number\n",
    "        file_id = file['file_id']\n",
    "        file_name = file['file_name']\n",
    "        file_number = file['file_number']\n",
    "\n",
    "        # Format the file_number as a six-digit string\n",
    "        number_prefix = f\"{file_number:06d}\"\n",
    "\n",
    "        # Remove the file extension from the file_name if it exists\n",
    "        file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "\n",
    "        # Define the new folder name and path in GCS\n",
    "        new_folder_name = f\"{number_prefix}_{file_name_no_ext}\"\n",
    "        new_folder_path = f\"{processed_folder}/{new_folder_name}\"\n",
    "\n",
    "        # Define the new blob path in GCS\n",
    "        copied_filename = f\"{file_name_no_ext}.pdf\"\n",
    "        new_blob = bucket.blob(f\"{new_folder_path}/{number_prefix}_{copied_filename}\")\n",
    "\n",
    "        # Initialize a BytesIO stream to hold the file data\n",
    "        fh = io.BytesIO()\n",
    "\n",
    "        if file['file_mime_type'] in [\n",
    "            'application/vnd.google-apps.document',\n",
    "            'application/vnd.google-apps.spreadsheet',\n",
    "            'application/vnd.google-apps.presentation',\n",
    "            'application/vnd.google-apps.drawing'\n",
    "        ]:\n",
    "            # Export Google Workspace files to PDF\n",
    "            request = drive_service.files().export_media(\n",
    "                fileId=file_id,\n",
    "                mimeType='application/pdf'\n",
    "                # 'export_media' does not support 'supportsAllDrives'\n",
    "            )\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Export {int(status.progress() * 100)}% complete.\")\n",
    "        elif file['file_mime_type'] in [\n",
    "            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "            'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
    "            'application/vnd.openxmlformats-officedocument.presentationml.slideshow',\n",
    "            'application/msword'\n",
    "        ]:\n",
    "            # Download Office file content\n",
    "            request = drive_service.files().get_media(fileId=file_id, supportsAllDrives=True)\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Download {int(status.progress() * 100)}% complete.\")\n",
    "\n",
    "            # Convert Office file to Google Workspace format by uploading it\n",
    "            mime_type_map = {\n",
    "                'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'application/vnd.google-apps.document',\n",
    "                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'application/vnd.google-apps.spreadsheet',\n",
    "                'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'application/vnd.google-apps.presentation',\n",
    "                'application/vnd.openxmlformats-officedocument.presentationml.slideshow': 'application/vnd.google-apps.presentation',\n",
    "                'application/msword': 'application/vnd.google-apps.document'\n",
    "            }\n",
    "            target_mime_type = mime_type_map.get(file['file_mime_type'])\n",
    "\n",
    "            if target_mime_type:\n",
    "                # Prepare the media upload object\n",
    "                fh.seek(0)  # Reset the stream position\n",
    "                media_body = MediaIoBaseUpload(\n",
    "                    fh,\n",
    "                    mimetype=file['file_mime_type'],\n",
    "                    resumable=False\n",
    "                )\n",
    "                # Upload the file to the service account's My Drive\n",
    "                uploaded_file = drive_service.files().create(\n",
    "                    body={\n",
    "                        'name': file_name_no_ext,  # Use file name without extension\n",
    "                        'mimeType': target_mime_type,\n",
    "                        'parents': []  # Ensure the file is uploaded to My Drive\n",
    "                    },\n",
    "                    media_body=media_body,\n",
    "                    fields='id'\n",
    "                    # Removed 'supportsAllDrives=True' since we're uploading to My Drive\n",
    "                ).execute()\n",
    "\n",
    "                # Export the uploaded file to PDF\n",
    "                fh = io.BytesIO()  # Reset the BytesIO object for the exported PDF\n",
    "                request = drive_service.files().export_media(\n",
    "                    fileId=uploaded_file['id'],\n",
    "                    mimeType='application/pdf'\n",
    "                    # 'export_media' does not support 'supportsAllDrives'\n",
    "                )\n",
    "                downloader = MediaIoBaseDownload(fh, request)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    status, done = downloader.next_chunk()\n",
    "                    print(f\"Export {int(status.progress() * 100)}% complete.\")\n",
    "\n",
    "                # Delete the uploaded file\n",
    "                drive_service.files().delete(fileId=uploaded_file['id']).execute()\n",
    "            else:\n",
    "                print(f\"Unsupported MIME type: {file['file_mime_type']}\")\n",
    "                return False\n",
    "        else:\n",
    "            # For PDFs and other files, download directly\n",
    "            request = drive_service.files().get_media(fileId=file_id, supportsAllDrives=True)\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Download {int(status.progress() * 100)}% complete.\")\n",
    "\n",
    "        # Upload the file content to GCS\n",
    "        fh.seek(0)\n",
    "        new_blob.upload_from_file(fh, content_type='application/pdf')\n",
    "        print(f\"File with ID {file_id} copied to {new_blob.name} in GCS bucket.\")\n",
    "\n",
    "        # Update BigQuery to set copied_to_gcs to TRUE and add the timestamp\n",
    "        paris_time = datetime.now(paris_tz)\n",
    "        timestamp_copied = paris_time.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        update_query = f\"\"\"\n",
    "        UPDATE `{table_id}`\n",
    "        SET copied_to_gcs = TRUE,\n",
    "            timestamp_copied = '{timestamp_copied}'\n",
    "        WHERE file_id = '{file_id}'\n",
    "        \"\"\"\n",
    "        with bq_lock:\n",
    "            update_job = bq_client.query(update_query)\n",
    "            update_job.result()\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file with ID {file_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "num_threads = 10  # Keep this or define again if you removed it\n",
    "\n",
    "max_runs = 5\n",
    "previous_count = None\n",
    "\n",
    "for run_index in range(1, max_runs + 1):\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Starting run {run_index} of {max_runs}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1) Re-run the same BigQuery query each time\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{table_id}`\n",
    "    WHERE copied_to_gcs = FALSE\n",
    "      AND file_created_time >= '2020-01-01'\n",
    "      AND (\n",
    "          file_mime_type = 'application/pdf'\n",
    "          OR file_mime_type IN (\n",
    "              'application/vnd.google-apps.document',\n",
    "              'application/vnd.google-apps.presentation',\n",
    "              'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
    "              'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
    "              'application/vnd.openxmlformats-officedocument.presentationml.slideshow',\n",
    "              'application/msword'\n",
    "          )\n",
    "      )\n",
    "    \"\"\"\n",
    "    query_job = bq_client.query(query)\n",
    "    files_to_copy = [dict(row) for row in query_job]\n",
    "    current_count = len(files_to_copy)\n",
    "\n",
    "    print(f\"Number of files found meeting the criteria: {current_count}\")\n",
    "\n",
    "    # 2) If no files left, stop\n",
    "    if current_count == 0:\n",
    "        print(\"No files left to process.\")\n",
    "        break\n",
    "\n",
    "    # 3) If the number of files hasn't decreased from the last run, stop\n",
    "    if previous_count is not None and current_count >= previous_count:\n",
    "        print(\"Number of files to process did not decrease. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # 4) Figure out how many files to process (batch_size or all)\n",
    "    files_to_process = files_to_copy if batch_size is None else files_to_copy[:batch_size]\n",
    "    print(f\"Number of files to process this run: {len(files_to_process)}\")\n",
    "\n",
    "    # 5) Use ThreadPoolExecutor to process them\n",
    "    total_files_copied_this_run = 0\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in files_to_process]\n",
    "        for future in as_completed(futures):\n",
    "            if future.result():\n",
    "                total_files_copied_this_run += 1\n",
    "\n",
    "    print(f\"Total files copied to GCS this run: {total_files_copied_this_run}\")\n",
    "\n",
    "    # 6) Update previous_count so we know if next run is making progress\n",
    "    previous_count = current_count\n",
    "\n",
    "print(\"Finished all runs or stopped because file count no longer decreases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99rtMXrjrqAU",
   "metadata": {
    "id": "99rtMXrjrqAU"
   },
   "source": [
    "# Document Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XrXRZrAO4J97",
   "metadata": {
    "id": "XrXRZrAO4J97"
   },
   "source": [
    "## Define Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KJ6Yr4VtiTjC",
   "metadata": {
    "id": "KJ6Yr4VtiTjC"
   },
   "outputs": [],
   "source": [
    "# Define prompt for extracting General summary of the document\n",
    "prompt_gen = \"\"\"\n",
    "1. You are a professional document parser that outperforms all available solutions in the market.\n",
    "\n",
    "2. Parse the uploaded PDF document and extract the following information:\n",
    "\n",
    "- Document Name: Extract the main title of the document, incorporating any relevant subtitles or captions that provide additional context. For example, if the document's title page has both a primary title and a secondary caption (e.g., \"Bild\" and \"American Elections\"), return a combined title such as \"Bild: American Elections\"\n",
    "- Document Date: The date of the document, if available.\n",
    "- Document Author: This could be an individual, company, organization, or the name of a publication such as a magazine or newspaper.\n",
    "- Document Type: Identify the type of document, which could be one of the following:\n",
    "  - Research Paper\n",
    "  - Report\n",
    "  - Presentation\n",
    "  - Publication\n",
    "  - Technical Documentation\n",
    "  - Other\n",
    "- Document Keywords: Extract and provide 15 keywords relevant to the document's content.\n",
    "- Document Summary: Generate a summary of the document. The summary should be 500 tokens in length\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fl_eGqmDU_b",
   "metadata": {
    "id": "8fl_eGqmDU_b"
   },
   "outputs": [],
   "source": [
    "# Define prompt for extracting Chunks\n",
    "prompt_chunks = \"\"\"\n",
    "<OBJECTIVE_AND_PERSONA>\n",
    "You are a professional multimodal document parser that outperforms all available solutions in the market like Unstructured.IO, PYPDF2, PYMUPDF, etc. Your task is to do the following:\n",
    "\n",
    "1. Parse through the uploaded PDF document and identify objects that constitute the document.\n",
    "\n",
    "The objects can be of the following CLASSES:\n",
    "- text\n",
    "\"text\" object is a text section of the article, paper, or presentation that has its own subtitle.\n",
    "- table\n",
    "\"table\" object is a structured arrangement of data organized into rows and columns.\n",
    "- chart\n",
    "\"chart\" object is a visual representation of data or information through graphical elements, such as bars, lines, pie slices, points, or curves to illustrate trends, patterns, comparisons, distributions, or correlations.\n",
    "- diagram\n",
    "\"diagram\" object is a visual representation that illustrates the structure, relationships, or workings of concepts, processes, or systems, using shapes, symbols, and lines to simplify complex information.\n",
    "- image\n",
    "\"image\" object is any visual representation, excluding \"table,\" \"chart,\" \"graph,\" or \"diagram,\" that serves to convey information, enhance understanding, or provide visual interest related to the document's content. Before describing the images, classify them into two categories:\n",
    "- **Useful**\n",
    "- **Useless**\n",
    "\n",
    "**Useful images:** Useful images include, but are not limited to:\n",
    "   - **Diagrams, graphs, and charts** (e.g., displaying numerical data, technical processes)\n",
    "   - **Standalone tables embedded as images** (not tables embedded within graphs)\n",
    "   - **Mechanical systems or components** (e.g., auto parts, powertrain systems, motors, clutches, etc.)\n",
    "   - **Electrical systems** (e.g., inverters, OBCs, DCDC converters)\n",
    "   - **Technical mechanisms, concepts, and processes** (e.g., business or marketing processes)\n",
    "\n",
    "**Useless images:** Useless images do not provide relevant technical or informational value and **should be completely omitted from the output.** These include:\n",
    "   - Brand logos\n",
    "   - Decorative design elements\n",
    "   - Photos and images of people\n",
    "   - Photos and images of nature\n",
    "   - Photos and images of buildings\n",
    "   - **All representations of vehicles** (including photos, drawings, technical schematics, and diagrams of vehicles or their exterior)\n",
    "   - **Images on title or cover pages** (such as those labeled \"Cover Image,\" \"Title Page Image,\" or any images on the first page of a new section)\n",
    "\n",
    "2. Return the following output for each object one by one, sequentially, in a JSON array as per the provided <RESPONSE_SCHEMA>:\n",
    "\n",
    "For text and table objects, return the full text according to the <INSTRUCTIONS>.\n",
    "For chart, diagram, and useful image objects, return a detailed description according to the <INSTRUCTIONS>.\n",
    "\n",
    "3. Before finishing the generation:\n",
    "- Check whether your output text has covered all the objects present in the document:\n",
    "  - If yes, stop generation.\n",
    "  - If no, continue generation until you achieve the 8192 max token output limit.\n",
    "- Do not adapt the generation to fit all objects into the limit. Instead, try to return everything comprehensively and only stop when you reach the token limit or all objects have been processed.\n",
    "- Continue generating content up to the 8192-token limit, and stop at that point if all objects have not been covered yet.\n",
    "\n",
    "</OBJECTIVE_AND_PERSONA>\n",
    "\n",
    "<CONSTRAINTS>\n",
    "Stop generation only if the full text for all text and table objects in the document is returned, along with detailed descriptions of all chart, diagram, and useful image objects in the same document, or if 8192 output tokens are reached.\n",
    "</CONSTRAINTS>\n",
    "\n",
    "<INSTRUCTIONS>\n",
    "\n",
    "- For \"text\" object:\n",
    "\n",
    "1. **e_chunk_type** (string, enum, required):\n",
    "    - **Instruction**: Insert the type of the extracted object.\n",
    "    - **Example**: `\"text\"`\n",
    "\n",
    "2. **f_chunk_title** (string):\n",
    "    - **Instruction**: Identify the name of a section or a chapter that constitutes the object. If a name is present, use it directly. If a section lacks a title, generate a concise and descriptive name based on the content of that section.\n",
    "    - **Example**: `\"Motor Performance Over Time\"`\n",
    "\n",
    "3. **g_chunk_contents** (string):\n",
    "    - **Instruction**: Extract the full text of the section or the chapter that constitutes the object. Normalize any line breaks or hyphenations into complete words. If formulas or non-standard symbols appear, convert them into natural language descriptions. For example, represent 'y′=limΔx→0Δy/Δx' as 'the derivative of y with respect to x is equal to the limit as Δx approaches zero of the change in y divided by the change in x.'\n",
    "\n",
    "Replace special characters and notations with their full words (e.g., '&' as 'and', '@' as 'at'). Integrate footnotes and endnotes into the main text or summarize them if they provide additional information. If references to figures, tables, or graphs appear, such as 'See Figure 2' or 'as shown in Table 1,' interpret them contextually, especially if the visual elements are not available.\n",
    "\n",
    "Convert bullet points and numbered lists into continuous prose. Adjust the formatting of quotes and dialogue to match standard text flow. Remove any page numbers, headers, or footers that could disrupt the content. Rephrase textual references to equations, like 'Equation (3),' or integrate them into the surrounding text if the actual equations are not provided, and ensure equations are described fully in natural language.`\n",
    "\n",
    "4. **h_chunk_keywords** (array, items: string):\n",
    "    - **Instruction**: Generate a list of specific keywords that best represent the main topics, themes, or concepts of the given object. The keywords should be concise, relevant, and tailored to the content of the object.\n",
    "    - **Example**: `[\"brushless DC motor\", \"induction motor\", \"rotor dynamics\", \"torque output\", \"motor efficiency\"]`\n",
    "\n",
    "5. **i_chunk_summary** (string):\n",
    "    - **Instruction**: Generate a summary of the given object, keeping it concise and limited to no more than 80 tokens.\n",
    "    - **Example**: `\"The study explores motor performance factors, including practice conditions, environment, motivation, and physiological aspects like heart rate variability. Spaced practice and varied environments improve adaptability, while intrinsic motivation and feedback enhance progress. Sleep quality also plays a role in long-term skill retention, suggesting further research.\"`\n",
    "\n",
    "- For \"table\" object:\n",
    "\n",
    "1. **e_chunk_type** (string, enum, required):\n",
    "    - **Instruction**: Insert the type of the extracted object.\n",
    "    - **Example**: `\"table\"`\n",
    "\n",
    "2. **f_chunk_title** (string):\n",
    "    - **Instruction**: Identify the name of the object. If a name is present, use it directly. If a table lacks a title, generate a concise and descriptive name based on the content of the table.\n",
    "    - **Example**: `\"Comparative Characteristics of E-Axle Motors\"`\n",
    "\n",
    "3. **g_chunk_contents** (string):\n",
    "    - **Instruction**: Extract the data from the table and store it in a comma-separated format.\n",
    "    - **Example**: `\"Model, Power Output (kW), Torque (Nm), Efficiency (%), Weight (kg), Max Speed (RPM), Cooling Type, Price ($)\\nE-Motor X1000, 150, 320, 92, 85, 12,000, Liquid, 8,500\\nE-Motor S800, 120, 280, 90, 78, 11,500, Air, 7,200\\nE-Motor P750, 200, 360, 94, 95, 13,000, Liquid, 10,000\\nE-Motor R600, 100, 250, 88, 72, 10,000, Air, 6,500\\nE-Motor T900, 180, 340, 91, 88, 12,500, Liquid, 9,300\"`\n",
    "\n",
    "4. **h_chunk_keywords** (array, items: string):\n",
    "    - **Instruction**: Generate a list of specific keywords that best represent the main topics, themes, or concepts of the given object. The keywords should be concise, relevant, and tailored to the content of the object.\n",
    "    - **Example**: `[\"E-Axle motor\", \"power output\", \"torque comparison\", \"motor efficiency\", \"cooling type\"]`\n",
    "\n",
    "5. **i_chunk_summary** (string):\n",
    "    - **Instruction**: Generate a summary of the given object, keeping it concise and limited to no more than 80 tokens.\n",
    "    - **Example**: `\"The table compares various E-Axle motor models based on key characteristics, including power output, torque, efficiency, weight, maximum speed, cooling type, and price. It highlights differences in performance metrics and cooling methods, offering a quick reference for selecting suitable motor options.\"`\n",
    "\n",
    "- For \"chart\" object:\n",
    "\n",
    "1. **e_chunk_type** (string, enum, required):\n",
    "    - **Instruction**: Insert the type of the extracted object.\n",
    "    - **Example**: `\"chart\"`\n",
    "\n",
    "2. **f_chunk_title** (string):\n",
    "    - **Instruction**: Identify the name of the object. If a name is present, use it directly. If the object lacks a title, generate a concise and descriptive name based on the content of the object.\n",
    "    - **Example**: `\"Performance Analysis of Inverters Across Different Load Conditions\"`\n",
    "\n",
    "3. **g_chunk_contents** (string):\n",
    "    - **Instruction**: Describe all the data presented in the object, including every detail, numerical value, and characteristic without omitting any information. Ensure the description is comprehensive and covers each aspect of the data clearly.\n",
    "    - **Example**: `\"The bar chart titled **\\\"Performance Analysis of Inverters Across Different Load Conditions\\\"** presents data for four inverter models: Inverter A, Inverter B, Inverter C, and Inverter D. It compares two key metrics: Efficiency (%) and Power Output (kW).\\n\\n- **Inverter A** has an efficiency of **88%** and a power output of **50 kW**.\\n- **Inverter B** achieves a higher efficiency of **92%** with a power output of **150 kW**.\\n- **Inverter C** has an efficiency of **85%** and a power output of **300 kW**.\\n- **Inverter D** has the lowest efficiency among the four at **78%** but offers the highest power output of **400 kW**.\\n\\nEach bar represents these values clearly, showing the variation in both efficiency and power output across different models. The x-axis displays the inverter models, while the y-axis represents the numerical values for both metrics.\"`\n",
    "\n",
    "4. **h_chunk_keywords** (array, items: string):\n",
    "    - **Instruction**: Generate a list of specific keywords that best represent the main topics, themes, or concepts of the given object. The keywords should be concise, relevant, and tailored to the content of the object.\n",
    "    - **Example**: `[\"Inverter performance\", \"Efficiency comparison\", \"Power output\", \"Inverter models\", \"Load conditions\"]`\n",
    "\n",
    "5. **i_chunk_summary** (string):\n",
    "    - **Instruction**: Generate a summary of the given object, keeping it concise and limited to no more than 80 tokens.\n",
    "    - **Example**: `\"The chart compares four inverter models (A, B, C, D) based on efficiency (%) and power output (kW). Inverter B has the highest efficiency (92%), while Inverter D offers the highest power output (400 kW).\"`\n",
    "\n",
    "- For \"diagram\" object:\n",
    "\n",
    "1. **e_chunk_type** (string, enum, required):\n",
    "    - **Instruction**: Insert the type of the extracted object.\n",
    "    - **Example**: `\"diagram\"`\n",
    "\n",
    "2. **f_chunk_title** (string):\n",
    "    - **Instruction**: Identify the name of the object. If a name is present, use it directly. If the object lacks a title, generate a concise and descriptive name based on the content of the object.\n",
    "    - **Example**: `\"Operational Principle of a Hub E-Motor\"`\n",
    "\n",
    "3. **g_chunk_contents** (string):\n",
    "    - **Instruction**: Describe the principle and process shown in the object, including all details, numerical values, and characteristics. Ensure the description is thorough, covering every aspect without omitting any information.\n",
    "\n",
    "4. **h_chunk_keywords** (array, items: string):\n",
    "    - **Instruction**: Generate a list of specific keywords that best represent the main topics, themes, or concepts of the given object. The keywords should be concise, relevant, and tailored to the content of the object.\n",
    "    - **Example**: `[\"Hub E-Motor\", \"Stator\", \"Rotor\", \"Magnetic field\", \"Electric current\"]`\n",
    "\n",
    "5. **i_chunk_summary** (string):\n",
    "    - **Instruction**: Generate a summary of the given object, keeping it concise and limited to no more than 80 tokens.\n",
    "    - **Example**: `\"The diagram shows the operational principle of a hub E-motor, highlighting key components like the stator, rotor, and windings. It illustrates how electric current in the stator generates a magnetic field, creating torque that spins the rotor and drives the wheel directly.\"`\n",
    "\n",
    "- For \"image\" object:\n",
    "\n",
    "1. **e_chunk_type** (string, enum, required):\n",
    "    - **Instruction**: Insert the type of the extracted object.\n",
    "    - **Example**: `\"image\"`\n",
    "\n",
    "2. **f_chunk_title** (string):\n",
    "    - **Instruction**: Identify the name of the object. If a name is present, use it directly. If the object lacks a title, generate a concise and descriptive name of the object.\n",
    "    - **Example**: `\"Operational Principle of a Hub E-Motor\"`\n",
    "\n",
    "3. **g_chunk_contents** (string):\n",
    "    - **Instruction**: Describe the object, including all details, numerical values, and characteristics. Ensure the description is thorough, covering every aspect without omitting any information.\n",
    "\n",
    "4. **h_chunk_keywords** (array, items: string):\n",
    "    - **Instruction**: Generate a list of specific keywords that best represent the main topics, themes, or concepts of the given object. The keywords should be concise, relevant, and tailored to the content of the object.\n",
    "    - **Example**: `[\"Power Inverter\", \"DC-AC Conversion\", \"Circuit Board\", \"Capacitors\", \"Inductors\"]`\n",
    "\n",
    "5. **i_chunk_summary** (string):\n",
    "    - **Instruction**: Generate a short description of the given object, keeping it concise and limited to no more than 80 tokens.\n",
    "    - **Example**: `\"A power inverter designed for converting DC to AC power, featuring a metallic casing with cooling fins, exposed circuit board, and components like capacitors, inductors, and power transistors. It includes connection ports, status LEDs, and labels for easy identification. Suitable for regulating voltage in various electrical systems.\"`\n",
    "\n",
    "</INSTRUCTIONS>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pLOcb5pvGTF",
   "metadata": {
    "id": "3pLOcb5pvGTF"
   },
   "source": [
    "## Define Response Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L9Ds7XE6iZFZ",
   "metadata": {
    "id": "L9Ds7XE6iZFZ"
   },
   "outputs": [],
   "source": [
    "# Define the response schema for General prompt\n",
    "response_schema_gen = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"n_document_name\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"d_document_date\": {\n",
    "                \"type\": \"string\",\n",
    "                \"format\": \"date\"\n",
    "            },\n",
    "            \"o_document_author\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"p_document_type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\n",
    "                    \"Research Paper\",\n",
    "                    \"Report\",\n",
    "                    \"Presentation\",\n",
    "                    \"Publication\",\n",
    "                    \"Technical Documentation\",\n",
    "                    \"Other\"\n",
    "                ]\n",
    "            },\n",
    "            \"q_document_keywords\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"maxItems\": 15\n",
    "            },\n",
    "            \"r_document_summary\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"n_document_name\",\n",
    "            \"d_document_date\",\n",
    "            \"o_document_author\",\n",
    "            \"p_document_type\",\n",
    "            \"q_document_keywords\",\n",
    "            \"r_document_summary\"\n",
    "        ]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ITTqz7UhyTwo",
   "metadata": {
    "id": "ITTqz7UhyTwo"
   },
   "outputs": [],
   "source": [
    "# Define the response schema for Chunks prompt\n",
    "response_schema_chunks = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"e_chunk_type\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"text\", \"table\", \"chart\", \"graph\", \"diagram\", \"image\"]\n",
    "            },\n",
    "            \"f_chunk_title\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"g_chunk_contents\": {\n",
    "                \"type\": \"string\"\n",
    "            },\n",
    "            \"h_chunk_keywords\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"maxItems\": 5\n",
    "            },\n",
    "            \"i_chunk_summary\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"e_chunk_type\",\n",
    "            \"f_chunk_title\",\n",
    "            \"g_chunk_contents\",\n",
    "            \"h_chunk_keywords\",\n",
    "            \"i_chunk_summary\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xjzi-NZIvrND",
   "metadata": {
    "id": "xjzi-NZIvrND"
   },
   "source": [
    "## Define Generation Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p7VBlC3fijzR",
   "metadata": {
    "id": "p7VBlC3fijzR"
   },
   "outputs": [],
   "source": [
    "# Define generation configuration for General Prompt\n",
    "config_gen = GenerationConfig(\n",
    "    candidate_count=None,\n",
    "    stop_sequences=None,\n",
    "    max_output_tokens=8192,\n",
    "    temperature=None,\n",
    "    top_p=None,\n",
    "    top_k=None,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=response_schema_gen\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model_gen = GenerativeModel(model_name=\"gemini-1.5-flash-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XTCYL7BWxq2H",
   "metadata": {
    "id": "XTCYL7BWxq2H"
   },
   "outputs": [],
   "source": [
    "# Define generation configuration for Chunks Prompt\n",
    "config_chunks = GenerationConfig(\n",
    "    candidate_count=None,\n",
    "    stop_sequences=None,\n",
    "    max_output_tokens=8192,\n",
    "    temperature=1,\n",
    "    top_p=0.95,\n",
    "    top_k=None,\n",
    "    response_mime_type=\"application/json\",\n",
    "    response_schema=response_schema_chunks\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model_chunks = GenerativeModel(model_name=\"gemini-1.5-flash-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlDMCct41nXd",
   "metadata": {
    "id": "mlDMCct41nXd"
   },
   "source": [
    "## Define Supporting Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bev27xizwkKK",
   "metadata": {
    "id": "Bev27xizwkKK"
   },
   "source": [
    "### Process an entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MjnSuEiT1uHd",
   "metadata": {
    "id": "MjnSuEiT1uHd"
   },
   "outputs": [],
   "source": [
    "def process_pdf_gen(gs_uri, bucket_name):\n",
    "    # Initialize storage client within the thread to ensure thread safety\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Assign a unique session ID\n",
    "    aa_document_session_id = str(uuid.uuid4())\n",
    "\n",
    "    # Record start time\n",
    "    document_session_start = datetime.now(paris_tz)\n",
    "    ab_document_session_start = document_session_start.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "\n",
    "    # Initialize result dictionary\n",
    "    gen_result = {}\n",
    "\n",
    "    try:\n",
    "        # Load the generative model\n",
    "        model = GenerativeModel(model_name=\"gemini-1.5-flash-002\")\n",
    "\n",
    "        # Prepare the content\n",
    "        pdf_file_part = Part.from_uri(uri=gs_uri, mime_type=\"application/pdf\")\n",
    "        contents = [pdf_file_part, prompt_gen]\n",
    "\n",
    "        # Generate the content\n",
    "        response = model.generate_content(contents, generation_config=config_gen)\n",
    "\n",
    "        # Process the response\n",
    "        text_field = response.text.strip()\n",
    "        text_data = json.loads(text_field)\n",
    "        doc_data = text_data[0]\n",
    "\n",
    "        fields_to_extract = [\n",
    "            \"n_document_name\",\n",
    "            \"d_document_date\",\n",
    "            \"o_document_author\",\n",
    "            \"p_document_type\",\n",
    "            \"q_document_keywords\",\n",
    "            \"r_document_summary\"\n",
    "        ]\n",
    "\n",
    "        gen_result = {key: doc_data.get(key) for key in fields_to_extract}\n",
    "\n",
    "        # Additional fields\n",
    "        additional_fields = {\n",
    "            \"ai_document_finish_reason\": response.candidates[0].finish_reason.name,\n",
    "            \"ak_document_prompt_token_count\": response.usage_metadata.prompt_token_count,\n",
    "            \"am_document_candidates_token_count\": response.usage_metadata.candidates_token_count,\n",
    "            \"ao_document_total_token_count\": response.usage_metadata.total_token_count,\n",
    "            \"aq_document_model_version\": response._raw_response.model_version\n",
    "        }\n",
    "\n",
    "        gen_result.update(additional_fields)\n",
    "\n",
    "        # Record end time and calculate session duration\n",
    "        document_session_end = datetime.now(paris_tz)\n",
    "        ac_document_session_end = document_session_end.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "        session_duration = ac_document_session_end - ab_document_session_start\n",
    "        ad_document_session_duration = str(timedelta(seconds=session_duration.total_seconds())).split(\",\")[-1].strip()\n",
    "\n",
    "        # Add session information\n",
    "        gen_result.update({\n",
    "            \"aa_document_session_id\": aa_document_session_id,\n",
    "            \"ab_document_session_start\": ab_document_session_start.isoformat(),\n",
    "            \"ac_document_session_end\": ac_document_session_end.isoformat(),\n",
    "            \"ad_document_session_duration\": ad_document_session_duration\n",
    "        })\n",
    "\n",
    "        # Extract base folder and file name\n",
    "        base_folder = os.path.dirname(gs_uri.replace(f\"gs://{bucket_name}/\", \"\"))\n",
    "        file_name = os.path.basename(gs_uri)\n",
    "        file_base_name = file_name.rsplit('.', 1)[0]\n",
    "\n",
    "        # Save the JSON to GCS\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        output_file_name = f\"{file_base_name}_gen.json\"\n",
    "        output_path = f\"{base_folder}/{output_file_name}\"\n",
    "        output_blob = bucket.blob(output_path)\n",
    "        output_blob.upload_from_string(json.dumps(gen_result, indent=2), content_type='application/json')\n",
    "\n",
    "        print(f\"General LLM processing completed successfully for {gs_uri}\")\n",
    "\n",
    "        return gen_result, file_base_name, base_folder\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM processing: {e}\")\n",
    "\n",
    "        # Record end time and calculate session duration\n",
    "        document_session_end = datetime.now(paris_tz)\n",
    "        ac_document_session_end = document_session_end.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "        session_duration = ac_document_session_end - ab_document_session_start\n",
    "        ad_document_session_duration = str(timedelta(seconds=session_duration.total_seconds())).split(\",\")[-1].strip()\n",
    "\n",
    "        # Add session information to result\n",
    "        gen_result.update({\n",
    "            \"aa_document_session_id\": aa_document_session_id,\n",
    "            \"ab_document_session_start\": ab_document_session_start.isoformat(),\n",
    "            \"ac_document_session_end\": ac_document_session_end.isoformat(),\n",
    "            \"ad_document_session_duration\": ad_document_session_duration,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "        # Save the error information to GCS\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        output_file_name = f\"{file_base_name}_gen_error.json\"\n",
    "        output_path = f\"{base_folder}/{output_file_name}\"\n",
    "        output_blob = bucket.blob(output_path)\n",
    "        output_blob.upload_from_string(json.dumps(gen_result, indent=2), content_type='application/json')\n",
    "\n",
    "        return gen_result, file_base_name, base_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A5diakX7xEgN",
   "metadata": {
    "id": "A5diakX7xEgN"
   },
   "source": [
    "### Process page by page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lb4x6iNy11rm",
   "metadata": {
    "id": "Lb4x6iNy11rm"
   },
   "outputs": [],
   "source": [
    "def process_page(blob, bucket_name):\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    try:\n",
    "        page_file_uri = f\"gs://{bucket_name}/{blob.name}\"\n",
    "        page_file_name = os.path.basename(blob.name)\n",
    "\n",
    "        # Extract page number\n",
    "        page_number = page_file_name.split('_page_')[1].split('_')[0]\n",
    "\n",
    "        # Generate authenticated URL\n",
    "        page_file_url = f\"https://storage.cloud.google.com/{bucket_name}/{quote(blob.name)}\"\n",
    "\n",
    "        ae_chunks_session_id = str(uuid.uuid4())\n",
    "        chunks_session_start = datetime.now(paris_tz)\n",
    "        af_chunks_session_start = chunks_session_start.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "\n",
    "        print(f\"Processing file: {page_file_uri}\")\n",
    "\n",
    "        model_text = GenerativeModel(model_name=\"gemini-1.5-flash-002\")\n",
    "\n",
    "        # Prepare content for the LLM\n",
    "        pdf_file_part = Part.from_uri(uri=page_file_uri, mime_type=\"application/pdf\")\n",
    "        contents = [pdf_file_part, prompt_chunks]\n",
    "        response = model_text.generate_content(contents, generation_config=config_chunks)\n",
    "\n",
    "        text_field = response.text.strip()\n",
    "        text_data = json.loads(text_field)\n",
    "\n",
    "        page_chunks = []\n",
    "        for doc_data in text_data:\n",
    "            # Extract required fields\n",
    "            fields_to_extract = [\n",
    "                \"e_chunk_type\",\n",
    "                \"f_chunk_title\",\n",
    "                \"g_chunk_contents\",\n",
    "                \"h_chunk_keywords\",\n",
    "                \"i_chunk_summary\"\n",
    "            ]\n",
    "            chunk_result = {key: doc_data.get(key) for key in fields_to_extract}\n",
    "\n",
    "            # Record end time and calculate session duration\n",
    "            chunks_session_end = datetime.now(paris_tz)\n",
    "            ag_chunks_session_end = chunks_session_end.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "            chunks_session_duration = ag_chunks_session_end - af_chunks_session_start\n",
    "            ah_chunks_session_duration = str(timedelta(seconds=chunks_session_duration.total_seconds())).split(\",\")[-1].strip()\n",
    "\n",
    "            # Add additional fields\n",
    "            chunk_result.update({\n",
    "                \"a_chunk_id\": str(uuid.uuid4()),\n",
    "                \"b_page_number\": int(page_number),\n",
    "                \"aj_chunks_finish_reason\": response.candidates[0].finish_reason.name,\n",
    "                \"al_chunks_prompt_token_count\": response.usage_metadata.prompt_token_count,\n",
    "                \"an_chunks_candidates_token_count\": response.usage_metadata.candidates_token_count,\n",
    "                \"ap_chunks_total_token_count\": response.usage_metadata.total_token_count,\n",
    "                \"ar_chunks_model_version\": response._raw_response.model_version,\n",
    "                \"ae_chunks_session_id\": ae_chunks_session_id,\n",
    "                \"af_chunks_session_start\": af_chunks_session_start.isoformat(),\n",
    "                \"ag_chunks_session_end\": ag_chunks_session_end.isoformat(),\n",
    "                \"ah_chunks_session_duration\": ah_chunks_session_duration,\n",
    "                \"s_page_gsutil\": page_file_uri,\n",
    "                \"t_page_url\": page_file_url\n",
    "            })\n",
    "\n",
    "            # Ensure alphabetical order of keys\n",
    "            chunk_result = dict(sorted(chunk_result.items()))\n",
    "\n",
    "            page_chunks.append(chunk_result)\n",
    "\n",
    "        print(f\"Processed and extracted chunks for {page_file_uri}\")\n",
    "\n",
    "        return page_chunks\n",
    "\n",
    "    except Exception as page_error:\n",
    "        print(f\"Error processing {page_file_uri}: {page_error}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IotDIfedxXMa",
   "metadata": {
    "id": "IotDIfedxXMa"
   },
   "source": [
    "### Convert JSON to NDJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZb1FOyV14xt",
   "metadata": {
    "id": "RZb1FOyV14xt"
   },
   "outputs": [],
   "source": [
    "def convert_to_ndjson(json_data):\n",
    "    # Deserialize the JSON string into a Python list of dictionaries\n",
    "    python_data = json.loads(json_data)\n",
    "    # Convert the list of dictionaries to NDJSON\n",
    "    ndjson_lines = \"\\n\".join(json.dumps(record) for record in python_data)\n",
    "    return ndjson_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6TBcFghVxhBJ",
   "metadata": {
    "id": "6TBcFghVxhBJ"
   },
   "source": [
    "### Upload NDJON to Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Gmo0rdaNAFr",
   "metadata": {
    "id": "6Gmo0rdaNAFr"
   },
   "outputs": [],
   "source": [
    "def load_into_bigquery_from_gcs(gcs_uri):\n",
    "    try:\n",
    "        # Initialize BigQuery client\n",
    "        client = bigquery.Client()\n",
    "\n",
    "        dataset_id = \"your-project-id.YOUR_DATASET\"\n",
    "        table_id = f\"{dataset_id}.CHUNKS\"\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "            max_bad_records=10,            # Allow up to 10 bad records\n",
    "            ignore_unknown_values=False    # Fail on unknown fields\n",
    "        )\n",
    "\n",
    "        load_job = client.load_table_from_uri(\n",
    "            f'gs://{bucket_name}/{gcs_uri}',\n",
    "            table_id,\n",
    "            job_config=job_config\n",
    "        )\n",
    "\n",
    "        print(f\"Load job created (job ID: {load_job.job_id}). Waiting for it to complete...\")\n",
    "\n",
    "        # Optionally poll the job to show progress (instead of immediate load_job.result()).\n",
    "        while not load_job.done():\n",
    "            print(\"  Job still running...\")\n",
    "            time.sleep(5)  # Sleep or remove/adjust as needed\n",
    "\n",
    "        # If you prefer a direct block-until-complete approach (less chatty):\n",
    "        # load_job.result()\n",
    "\n",
    "        # --- Check job state ---\n",
    "        if load_job.state == \"DONE\":\n",
    "            if load_job.error_result:\n",
    "                # This means the job finished but had an overall error\n",
    "                print(f\"Job {load_job.job_id} completed with errors:\")\n",
    "                print(load_job.error_result)\n",
    "                # You might raise an Exception here to bubble it up if desired:\n",
    "                # raise RuntimeError(f\"BigQuery load job failed: {load_job.error_result}\")\n",
    "            else:\n",
    "                print(f\"Job {load_job.job_id} completed successfully!\")\n",
    "                # Check the loaded table’s row count\n",
    "                table = client.get_table(table_id)\n",
    "                print(f\"Total rows now in {table_id}: {table.num_rows}\")\n",
    "        else:\n",
    "            print(f\"Job {load_job.job_id} ended in state: {load_job.state}\")\n",
    "            if load_job.errors:\n",
    "                print(\"Detailed errors:\")\n",
    "                for err in load_job.errors:\n",
    "                    print(f\"  {err}\")\n",
    "            # You can raise an exception if you want to fail the script:\n",
    "            # raise RuntimeError(f\"BigQuery job ended in state {load_job.state}. Errors: {load_job.errors}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wExJZmS_xmyy",
   "metadata": {
    "id": "wExJZmS_xmyy"
   },
   "source": [
    "### Main processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biLn7yEtlpK2",
   "metadata": {
    "id": "biLn7yEtlpK2"
   },
   "outputs": [],
   "source": [
    "def process_file(file_number):\n",
    "    # Derive number_prefix from file_number\n",
    "    number_prefix = f\"{int(file_number):06d}\"\n",
    "\n",
    "    # Initialize per-file variables\n",
    "    metadata = {}\n",
    "    gen_result = {}\n",
    "    all_chunks = []\n",
    "    base_folder = ''\n",
    "    file_base_name = ''\n",
    "    processing_failed = False  # Initialize failure flag\n",
    "    status = 'parsed'  # Default status\n",
    "\n",
    "    # Record start time\n",
    "    paris_tz = ZoneInfo(\"Europe/Paris\")\n",
    "    paris_time = datetime.now(paris_tz)\n",
    "    session_start = paris_time.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "\n",
    "    try:\n",
    "        # **Step 1: Programmatic File Processing**\n",
    "\n",
    "        # Initialize storage client within the function\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Find the folder that starts with number_prefix\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=base_gcs_folder + '/', delimiter='/')\n",
    "        folder_prefixes = set()\n",
    "        for page in blobs.pages:\n",
    "            folder_prefixes.update(page.prefixes)\n",
    "\n",
    "        folder_name = None\n",
    "        for prefix in folder_prefixes:\n",
    "            folder = prefix.rstrip('/').split('/')[-1]\n",
    "            if folder.startswith(number_prefix + '_'):\n",
    "                folder_name = folder\n",
    "                break\n",
    "\n",
    "        if not folder_name:\n",
    "            print(f\"No folder found for number_prefix {number_prefix}\")\n",
    "            status = 'failed'\n",
    "\n",
    "            # Return failure status\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': None,\n",
    "                'timestamp_parsed': datetime.now(paris_tz).replace(tzinfo=None).isoformat()\n",
    "            }\n",
    "\n",
    "        folder_path = f\"{base_gcs_folder}/{folder_name}\"\n",
    "\n",
    "        print(f\"Processing folder {folder_path}...\")\n",
    "\n",
    "        # Now list all blobs under that folder\n",
    "        blobs = storage_client.list_blobs(bucket_name, prefix=folder_path + '/')\n",
    "\n",
    "        file_blob = None\n",
    "        for blob in blobs:\n",
    "            blob_name = blob.name.split('/')[-1]\n",
    "            if blob_name.startswith(number_prefix + '_'):\n",
    "                file_blob = blob\n",
    "                break\n",
    "\n",
    "        if not file_blob:\n",
    "            print(f\"No file found in folder {folder_path} starting with {number_prefix}_\")\n",
    "            status = 'failed'\n",
    "\n",
    "            # Return failure status\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': None,\n",
    "                'timestamp_parsed': datetime.now(paris_tz).replace(tzinfo=None).isoformat()\n",
    "            }\n",
    "\n",
    "        # Now we have the file_blob to process\n",
    "        source_blob = file_blob\n",
    "        source_filename = source_blob.name.split('/')[-1]\n",
    "        filename_without_ext = os.path.splitext(source_filename)[0]\n",
    "        local_file_name = source_filename\n",
    "        local_file_path = f\"/tmp/{local_file_name}\"\n",
    "\n",
    "        # Check file extension\n",
    "        file_extension = os.path.splitext(local_file_name)[1].lower()\n",
    "        if file_extension != '.pdf':\n",
    "            print(f\"File {local_file_name} is not a PDF. Skipping.\")\n",
    "            status = 'failed'\n",
    "\n",
    "            # Return failure status\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': None,\n",
    "                'timestamp_parsed': datetime.now(paris_tz).replace(tzinfo=None).isoformat()\n",
    "            }\n",
    "\n",
    "        # Download the file locally for processing\n",
    "        source_blob.download_to_filename(local_file_path)\n",
    "\n",
    "        print(f\"Processing file {source_filename}...\")\n",
    "\n",
    "        # Open the PDF with PyMuPDF and analyze it\n",
    "        doc = pymupdf.open(local_file_path)\n",
    "        num_pages = doc.page_count\n",
    "        layout = \"landscape\" if doc[0].rect.width > doc[0].rect.height else \"portrait\"\n",
    "\n",
    "        # Adjust page numbering format based on total pages\n",
    "        num_digits = len(str(num_pages))\n",
    "\n",
    "        # Create local directory for pages with prefix included\n",
    "        pages_folder_name = f\"{filename_without_ext}_pages\"\n",
    "        local_pages_folder = f\"/tmp/{pages_folder_name}\"\n",
    "        os.makedirs(local_pages_folder, exist_ok=True)\n",
    "\n",
    "        # Save each page as a separate PDF file and upload to GCS\n",
    "        for i in tqdm(range(num_pages), desc=f'Processing {source_filename}', unit='page'):\n",
    "            try:\n",
    "                # Format page number based on total pages\n",
    "                page_num_str = f\"{i+1:0{num_digits}d}\"\n",
    "                page_filename = f\"{filename_without_ext}_page_{page_num_str}_{num_pages}.pdf\"\n",
    "                local_page_path = f\"{local_pages_folder}/{page_filename}\"\n",
    "                # GCS pages folder\n",
    "                gcs_page_path = f\"{folder_path}/{pages_folder_name}/{page_filename}\"\n",
    "\n",
    "                # Create a new document with just this page\n",
    "                single_page_doc = pymupdf.open()\n",
    "                single_page_doc.insert_pdf(doc, from_page=i, to_page=i)\n",
    "                single_page_doc.save(local_page_path)\n",
    "                single_page_doc.close()\n",
    "\n",
    "                # Upload the single-page PDF to GCS\n",
    "                page_blob = bucket.blob(gcs_page_path)\n",
    "                page_blob.upload_from_filename(local_page_path, content_type='application/pdf')\n",
    "\n",
    "                # Optionally, delete the local page file to save space\n",
    "                os.remove(local_page_path)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing page {i+1} of {source_filename}: {e}\")\n",
    "                processing_failed = True\n",
    "                status = 'failed'\n",
    "                break  # Stop processing pages if one fails\n",
    "\n",
    "        if processing_failed:\n",
    "            print(f\"Processing of pages failed for {source_filename}\")\n",
    "\n",
    "        # Close the main document\n",
    "        doc.close()\n",
    "\n",
    "        # Calculate SHA-256 hash of the file\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(local_file_path, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "\n",
    "        # Record end time and calculate session duration\n",
    "        paris_time_end = datetime.now(paris_tz)\n",
    "        session_end = paris_time_end.replace(tzinfo=None)  # Strip time zone info to keep local Paris time\n",
    "        session_duration = session_end - session_start\n",
    "        duration_str = str(session_duration)\n",
    "\n",
    "        # Construct the authenticated URL using the blob's properties\n",
    "        authenticated_url = f\"https://storage.cloud.google.com/{bucket_name}/{quote(source_blob.name)}\"\n",
    "\n",
    "        # Construct the gsutil URI\n",
    "        gsutil_uri = f\"gs://{bucket_name}/{source_blob.name}\"\n",
    "\n",
    "        # Save JSON metadata\n",
    "        metadata = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"j_file_number\": int(number_prefix),\n",
    "                \"k_file_hash\": sha256_hash.hexdigest(),\n",
    "                \"l_file_name\": filename_without_ext,\n",
    "                \"u_file_gsutil\": gsutil_uri,\n",
    "                \"v_file_url\": authenticated_url,\n",
    "                \"m_file_layout\": layout,\n",
    "                \"c_file_pages\": num_pages,\n",
    "                \"w_file_session_start\": session_start.isoformat(),\n",
    "                \"x_file_session_end\": session_end.isoformat(),\n",
    "                \"y_file_session_duration\": duration_str,\n",
    "                \"z_file_session_status\": \"done\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "        json_filename = f\"{filename_without_ext}_prep.json\"\n",
    "        json_path = f\"{folder_path}/{json_filename}\"\n",
    "\n",
    "        # Save JSON to GCS\n",
    "        try:\n",
    "            json_blob = bucket.blob(json_path)\n",
    "            json_blob.upload_from_string(json.dumps(metadata, indent=4), content_type='application/json')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while saving JSON metadata to GCS for {source_filename}: {e}\")\n",
    "            processing_failed = True\n",
    "            status = 'failed'\n",
    "\n",
    "        # Clean up local files\n",
    "        os.remove(local_file_path)\n",
    "        os.rmdir(local_pages_folder)\n",
    "\n",
    "        print(f\"Programmatic processing of {source_filename} completed successfully.\\n\")\n",
    "\n",
    "        # **Step 2: Concurrent LLM Processing**\n",
    "\n",
    "        # Extract base folder and file name\n",
    "        base_folder = os.path.dirname(gsutil_uri.replace(f\"gs://{bucket_name}/\", \"\"))\n",
    "        file_name = os.path.basename(gsutil_uri)\n",
    "        file_base_name = file_name.rsplit('.', 1)[0]\n",
    "        pages_folder = f\"{base_folder}/{filename_without_ext}_pages\"\n",
    "\n",
    "        # Initialize storage client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blobs = list(bucket.list_blobs(prefix=pages_folder))\n",
    "\n",
    "        if not blobs:\n",
    "            print(f\"No files found in the folder: {pages_folder}\")\n",
    "            processing_failed = True\n",
    "            status = 'failed'\n",
    "\n",
    "            # Return failure status\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': None,\n",
    "                'timestamp_parsed': datetime.now(paris_tz).replace(tzinfo=None).isoformat()\n",
    "            }\n",
    "\n",
    "        # Filter and sort blobs\n",
    "        blobs = [blob for blob in sorted(blobs, key=lambda x: x.name) if blob.name.endswith('.pdf')]\n",
    "\n",
    "        # Prepare tasks\n",
    "        tasks = []\n",
    "\n",
    "        # Add the process_pdf_gen task\n",
    "        tasks.append(('gen', gsutil_uri))\n",
    "\n",
    "        # Add per-page processing tasks\n",
    "        for blob in blobs:\n",
    "            tasks.append(('page', blob))\n",
    "\n",
    "        # Process tasks concurrently\n",
    "        total_tasks = len(tasks)\n",
    "\n",
    "        # Use a ThreadPoolExecutor with max_workers set to num_pages + 1 (for 'gen' task)\n",
    "        with ThreadPoolExecutor(max_workers=total_tasks) as executor:\n",
    "            future_to_task = {}\n",
    "            for task in tasks:\n",
    "                if task[0] == 'gen':\n",
    "                    # Submit the process_pdf_gen function\n",
    "                    future = executor.submit(process_pdf_gen, task[1], bucket_name)\n",
    "                elif task[0] == 'page':\n",
    "                    # Submit the process_page function\n",
    "                    future = executor.submit(process_page, task[1], bucket_name)\n",
    "                future_to_task[future] = task\n",
    "\n",
    "            for future in as_completed(future_to_task):\n",
    "                task = future_to_task[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if task[0] == 'gen':\n",
    "                        gen_result_local, file_base_name_local, base_folder_local = result\n",
    "                        if not gen_result_local:\n",
    "                            processing_failed = True\n",
    "                            status = 'failed'\n",
    "                            print(f\"Processing of general summary failed for {source_filename}\")\n",
    "                        else:\n",
    "                            gen_result = gen_result_local\n",
    "                            file_base_name = file_base_name_local\n",
    "                            base_folder = base_folder_local\n",
    "                    elif task[0] == 'page':\n",
    "                        if result:\n",
    "                            all_chunks.extend(result)\n",
    "                        else:\n",
    "                            processing_failed = True\n",
    "                            status = 'failed'\n",
    "                            print(f\"Processing of page failed for {task[1].name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An exception occurred while processing task {task}: {e}\")\n",
    "                    processing_failed = True\n",
    "                    status = 'failed'\n",
    "\n",
    "        if not processing_failed:\n",
    "            # **Save all_chunks before merging**\n",
    "            chunks_output_file_name = f\"{file_base_name}_chunks.json\"\n",
    "            chunks_output_path = f\"{base_folder}/{chunks_output_file_name}\"\n",
    "\n",
    "            try:\n",
    "                chunks_blob = bucket.blob(chunks_output_path)\n",
    "                chunks_blob.upload_from_string(json.dumps(all_chunks, indent=2), content_type='application/json')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving chunks JSON to GCS for {source_filename}: {e}\")\n",
    "                processing_failed = True\n",
    "                status = 'failed'\n",
    "\n",
    "            # **Merge Metadata and Results**\n",
    "\n",
    "            final_chunks = all_chunks\n",
    "            sorted_chunks = []\n",
    "\n",
    "            cleaned_metadata = metadata.get('properties', {})\n",
    "\n",
    "            for chunk in final_chunks:\n",
    "                chunk.update(cleaned_metadata)  # Merge metadata fields into the chunk\n",
    "                chunk.update(gen_result)        # Merge result fields into the chunk\n",
    "\n",
    "                # Define a sorting function that extracts the prefix before the underscore\n",
    "                def sort_key(item):\n",
    "                    key = item[0]\n",
    "                    prefix = key.split('_')[0]\n",
    "                    return (len(prefix), prefix)\n",
    "\n",
    "                # Sort the keys using the custom sort function\n",
    "                sorted_chunk = OrderedDict(sorted(chunk.items(), key=sort_key))\n",
    "                sorted_chunks.append(sorted_chunk)\n",
    "\n",
    "            # Ensure 'b_page_number' is present and is an integer\n",
    "            sorted_chunks = sorted(sorted_chunks, key=lambda chunk: int(chunk.get('b_page_number', 0)))\n",
    "\n",
    "            # Convert the updated chunks to JSON without sorting the keys\n",
    "            merged_json = json.dumps(sorted_chunks, indent=2)\n",
    "\n",
    "            # Save JSON to GCS\n",
    "            output_file_name = f\"{file_base_name}_merged.json\"\n",
    "            output_path = f\"{base_folder}/{output_file_name}\"\n",
    "\n",
    "            # Create a blob and upload the JSON string\n",
    "            try:\n",
    "                output_blob = bucket.blob(output_path)\n",
    "                output_blob.upload_from_string(merged_json, content_type='application/json')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while saving merged JSON to GCS for {source_filename}: {e}\")\n",
    "                processing_failed = True\n",
    "                status = 'failed'\n",
    "\n",
    "            print(f\"Saved merged JSON to {output_path}\")\n",
    "\n",
    "            # **Convert JSON to NDJSON**\n",
    "\n",
    "            # Convert JSON string to NDJSON\n",
    "            ndjson_data = convert_to_ndjson(merged_json)\n",
    "\n",
    "            if not ndjson_data:\n",
    "                print(f\"Conversion to NDJSON failed for {source_filename}\")\n",
    "                processing_failed = True\n",
    "                status = 'failed'\n",
    "            else:\n",
    "                # Save NDJSON to GCS\n",
    "                output_file_name_nd = f\"{file_base_name}_merged_nd.json\"\n",
    "                output_path_nd = f\"{base_folder}/{output_file_name_nd}\"\n",
    "\n",
    "                try:\n",
    "                    output_blob_nd = bucket.blob(output_path_nd)\n",
    "                    output_blob_nd.upload_from_string(ndjson_data, content_type='application/json')\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while saving NDJSON to GCS for {source_filename}: {e}\")\n",
    "                    processing_failed = True\n",
    "                    status = 'failed'\n",
    "\n",
    "                print(f\"Saved merged NDJSON to {output_path_nd}\")\n",
    "\n",
    "            # After merging, record timestamp_parsed\n",
    "            timestamp_parsed = datetime.now(paris_tz).replace(tzinfo=None)  # Strip timezone info\n",
    "\n",
    "            # Return the necessary information for later processing\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': output_path_nd if not processing_failed else None,\n",
    "                'timestamp_parsed': timestamp_parsed.isoformat()\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping merging due to earlier failures for {source_filename}\")\n",
    "            timestamp_parsed = datetime.now(paris_tz).replace(tzinfo=None)\n",
    "\n",
    "            # Return failure status with timestamp_parsed\n",
    "            return {\n",
    "                'file_number': file_number,\n",
    "                'status': status,\n",
    "                'ndjson_gcs_path': None,\n",
    "                'timestamp_parsed': timestamp_parsed.isoformat()\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing file number {file_number}: {e}\")\n",
    "        processing_failed = True\n",
    "        status = 'failed'\n",
    "        timestamp_parsed = datetime.now(paris_tz).replace(tzinfo=None)\n",
    "\n",
    "        # Handle exception, update the metadata if necessary\n",
    "        return {\n",
    "            'file_number': file_number,\n",
    "            'status': status,\n",
    "            'ndjson_gcs_path': None,\n",
    "            'timestamp_parsed': timestamp_parsed.isoformat()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QDbrPiKrwdjM",
   "metadata": {
    "id": "QDbrPiKrwdjM"
   },
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ATrC7Nps_yjz",
   "metadata": {
    "id": "ATrC7Nps_yjz"
   },
   "outputs": [],
   "source": [
    "#------SCRIPT FOR CHECKING THE ELIGIBLE FILES AND SETTING UP A LOOP-------------------------\n",
    "\n",
    "# --- SNIPPET FOR SETTING A LOOP 1. Start\n",
    "\n",
    "MAX_RUNS = 5  # Number of maximum runs you want; change as needed\n",
    "run_number = 0\n",
    "previous_files_count = None\n",
    "\n",
    "while True:\n",
    "    run_number += 1\n",
    "    print(f\"\\n========== RUN #{run_number}/{MAX_RUNS} ==========\\n\")\n",
    "\n",
    "    # --- SNIPPET FOR SETTING A LOOP 1. End\n",
    "\n",
    "    # Initialize Google Cloud clients\n",
    "    storage_client = storage.Client()\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Set Paris timezone\n",
    "    paris_tz = ZoneInfo(\"Europe/Paris\")\n",
    "\n",
    "    # Configurable parameter: Maximum files to process in this run\n",
    "    MAX_FILES_TO_PROCESS = None  # Change to control number of files processed in one run\n",
    "\n",
    "    # Base GCS folder and bucket name\n",
    "    base_gcs_folder = 'Sample Drive Folder'\n",
    "    bucket_name = 'your-project-id-knowledge-base'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Query BigQuery to get all files where 'parsed' is FALSE\n",
    "    query = \"\"\"\n",
    "    SELECT file_number\n",
    "    FROM `your-project-id.YOUR_DATASET.FILE_LIST`\n",
    "    WHERE copied_to_gcs = TRUE\n",
    "      AND parsed IN ('false', 'failed')\n",
    "    \"\"\"\n",
    "    query_job = bq_client.query(query)\n",
    "    results = query_job.result()\n",
    "\n",
    "    # Get all eligible file_numbers as integers\n",
    "    eligible_files = [int(row.file_number) for row in results]\n",
    "    total_eligible_files = len(eligible_files)\n",
    "\n",
    "    # Print the number of eligible files\n",
    "    print(f\"Total number of eligible files: {total_eligible_files}\")\n",
    "\n",
    "    # Apply the processing limit if specified\n",
    "    if MAX_FILES_TO_PROCESS is not None:\n",
    "        files_to_process = eligible_files[:MAX_FILES_TO_PROCESS]\n",
    "    else:\n",
    "        files_to_process = eligible_files  # Process all files if no limit is set\n",
    "\n",
    "    print(f\"Number of files to be processed in this run: {len(files_to_process)}\")\n",
    "\n",
    "    #------MAIN SCRIPT-------------------------\n",
    "\n",
    "    # Initialize counters and script start time\n",
    "    total_files_processed = 0\n",
    "    script_start_time = datetime.now(paris_tz)\n",
    "\n",
    "    API_LIMIT = 100  # Your API limit for concurrent tasks\n",
    "\n",
    "    # Configurable parameter: Pause duration between batches in seconds\n",
    "    BATCH_PAUSE_SECONDS = 60  # Set to 0 to disable pause between batches\n",
    "\n",
    "    # Collect page counts for each file without using a separate function\n",
    "    file_page_counts = []\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    for file_number in files_to_process:\n",
    "        try:\n",
    "            # Derive number_prefix from file_number\n",
    "            number_prefix = f\"{int(file_number):06d}\"\n",
    "\n",
    "            # Find the folder that starts with number_prefix\n",
    "            blobs = storage_client.list_blobs(bucket_name, prefix=base_gcs_folder + '/', delimiter='/')\n",
    "            folder_prefixes = set()\n",
    "            for page in blobs.pages:\n",
    "                folder_prefixes.update(page.prefixes)\n",
    "\n",
    "            folder_name = None\n",
    "            for prefix in folder_prefixes:\n",
    "                folder = prefix.rstrip('/').split('/')[-1]\n",
    "                if folder.startswith(number_prefix + '_'):\n",
    "                    folder_name = folder\n",
    "                    break\n",
    "\n",
    "            if not folder_name:\n",
    "                print(f\"No folder found for number_prefix {number_prefix}\")\n",
    "                continue  # Skip to next file\n",
    "\n",
    "            folder_path = f\"{base_gcs_folder}/{folder_name}\"\n",
    "\n",
    "            # Now list all blobs under that folder\n",
    "            blobs = storage_client.list_blobs(bucket_name, prefix=folder_path + '/')\n",
    "\n",
    "            file_blob = None\n",
    "            for blob in blobs:\n",
    "                blob_name = blob.name.split('/')[-1]\n",
    "                if blob_name.startswith(number_prefix + '_'):\n",
    "                    file_blob = blob\n",
    "                    break\n",
    "\n",
    "            if not file_blob:\n",
    "                print(f\"No file found in folder {folder_path} starting with {number_prefix}_\")\n",
    "                continue  # Skip to next file\n",
    "\n",
    "            # Now we have the file_blob to process\n",
    "            source_blob = file_blob\n",
    "            source_filename = source_blob.name.split('/')[-1]\n",
    "            local_file_name = source_filename\n",
    "            local_file_path = f\"/tmp/{local_file_name}\"\n",
    "\n",
    "            # Check file extension\n",
    "            file_extension = os.path.splitext(local_file_name)[1].lower()\n",
    "            if file_extension != '.pdf':\n",
    "                print(f\"File {local_file_name} is not a PDF. Skipping.\")\n",
    "                continue  # Skip to next file\n",
    "\n",
    "            # Download the file locally to read page count\n",
    "            source_blob.download_to_filename(local_file_path)\n",
    "\n",
    "            # Open the PDF with PyMuPDF and get the page count\n",
    "            doc = pymupdf.open(local_file_path)\n",
    "            num_pages = doc.page_count\n",
    "            doc.close()\n",
    "\n",
    "            # Clean up local file\n",
    "            os.remove(local_file_path)\n",
    "\n",
    "            # Append to the list\n",
    "            file_page_counts.append((file_number, num_pages))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while getting page count for file number {file_number}: {e}\")\n",
    "            continue  # Skip to next file\n",
    "\n",
    "    # Batch files based on total page counts not exceeding API_LIMIT\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_batch_page_count = 0\n",
    "\n",
    "    for file_number, num_pages in file_page_counts:\n",
    "        if current_batch_page_count + num_pages <= API_LIMIT:\n",
    "            current_batch.append(file_number)\n",
    "            current_batch_page_count += num_pages\n",
    "        else:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = [file_number]\n",
    "            current_batch_page_count = num_pages\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    # Initialize a list to collect processing results\n",
    "    processing_results = []\n",
    "\n",
    "    # Process files in batches\n",
    "    for batch_num, batch in enumerate(batches, start=1):\n",
    "        print(f\"Processing batch {batch_num}/{len(batches)} with files: {batch}\")\n",
    "\n",
    "        # Number of concurrent file workers is the number of files in the batch\n",
    "        max_file_workers = len(batch)\n",
    "\n",
    "        # Process files concurrently within the batch\n",
    "        with ThreadPoolExecutor(max_workers=max_file_workers) as executor:\n",
    "            future_to_file_number = {\n",
    "                executor.submit(process_file, file_number): file_number for file_number in batch\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_file_number):\n",
    "                file_number = future_to_file_number[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    processing_results.append(result)\n",
    "                    if result['status'] == 'parsed':\n",
    "                        total_files_processed += 1  # Increment processed files counter\n",
    "                    else:\n",
    "                        print(f\"Processing failed for file number {file_number}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"File {file_number} generated an exception: {e}\")\n",
    "\n",
    "        # Pause between batches if BATCH_PAUSE_SECONDS is greater than 0 and not the last batch\n",
    "        if BATCH_PAUSE_SECONDS > 0 and batch_num < len(batches):\n",
    "            print(f\"Pausing for {BATCH_PAUSE_SECONDS} seconds before processing the next batch...\")\n",
    "            time.sleep(BATCH_PAUSE_SECONDS)\n",
    "\n",
    "    # === New Code Start ===\n",
    "    # After processing all files, proceed to load data into BigQuery and update statuses\n",
    "\n",
    "    # Collect status updates\n",
    "    status_updates = []\n",
    "\n",
    "    for result in processing_results:\n",
    "        file_number = result['file_number']\n",
    "        status = result['status']\n",
    "        timestamp_parsed = result.get('timestamp_parsed', datetime.now(paris_tz).replace(tzinfo=None).isoformat())\n",
    "        status_updates.append({\n",
    "            'file_number': file_number,\n",
    "            'parsed': status,\n",
    "            'timestamp_parsed': timestamp_parsed\n",
    "        })\n",
    "\n",
    "    # Save status updates to JSON and NDJSON\n",
    "    status_updates_json = json.dumps(status_updates, indent=2)\n",
    "    status_updates_ndjson = '\\n'.join(json.dumps(record) for record in status_updates)\n",
    "\n",
    "    # Save to GCS\n",
    "    timestamp_str = datetime.now(paris_tz).strftime(\"%Y%m%d%H%M%S\")\n",
    "    status_json_path = f\"{base_gcs_folder}/!status_json/status_{timestamp_str}.json\"\n",
    "    status_ndjson_path = f\"{base_gcs_folder}/!status_json/status_{timestamp_str}.ndjson\"\n",
    "\n",
    "    status_blob_json = bucket.blob(status_json_path)\n",
    "    status_blob_ndjson = bucket.blob(status_ndjson_path)\n",
    "\n",
    "    status_blob_json.upload_from_string(status_updates_json, content_type='application/json')\n",
    "    status_blob_ndjson.upload_from_string(status_updates_ndjson, content_type='application/json')\n",
    "\n",
    "    # Load the status NDJSON into a temporary BigQuery table\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"file_number\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"parsed\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"timestamp_parsed\", \"DATETIME\"),\n",
    "        ],\n",
    "    )\n",
    "    tmp_table_id = \"your-project-id.YOUR_DATASET._tmp_status_updates\"\n",
    "\n",
    "    load_job = bq_client.load_table_from_uri(\n",
    "        f'gs://{bucket_name}/{status_ndjson_path}',\n",
    "        tmp_table_id,\n",
    "        job_config=job_config\n",
    "    )\n",
    "\n",
    "    load_job.result()\n",
    "\n",
    "    # Perform a MERGE operation to update the main table\n",
    "    merge_query = \"\"\"\n",
    "    MERGE `your-project-id.YOUR_DATASET.FILE_LIST` T\n",
    "    USING `your-project-id.YOUR_DATASET._tmp_status_updates` S\n",
    "    ON T.file_number = S.file_number\n",
    "    WHEN MATCHED THEN\n",
    "      UPDATE SET\n",
    "        T.parsed = S.parsed,\n",
    "        T.timestamp_parsed = S.timestamp_parsed\n",
    "    \"\"\"\n",
    "\n",
    "    query_job = bq_client.query(merge_query)\n",
    "    query_job.result()\n",
    "\n",
    "    # Optionally, delete the temporary table\n",
    "    bq_client.delete_table(tmp_table_id, not_found_ok=True)\n",
    "\n",
    "    # Concatenate NDJSON files and load into BigQuery\n",
    "    ndjson_contents = []\n",
    "    successful_file_numbers = []\n",
    "\n",
    "    for result in processing_results:\n",
    "        ndjson_gcs_path = result['ndjson_gcs_path']\n",
    "        status = result['status']\n",
    "        file_number = result['file_number']\n",
    "\n",
    "        if ndjson_gcs_path and status == 'parsed':\n",
    "            ndjson_blob = bucket.blob(ndjson_gcs_path)\n",
    "            ndjson_data = ndjson_blob.download_as_text()\n",
    "            ndjson_contents.append(ndjson_data)\n",
    "            successful_file_numbers.append(file_number)\n",
    "\n",
    "    # Combine and save the combined NDJSON to GCS\n",
    "    if ndjson_contents:\n",
    "        combined_ndjson_data = '\\n'.join(ndjson_contents)\n",
    "        timestamp_str = datetime.now(paris_tz).strftime(\"%Y%m%d%H%M%S\")\n",
    "        combined_ndjson_path = f\"{base_gcs_folder}/!session_json/session_{timestamp_str}.json\"\n",
    "        combined_ndjson_blob = bucket.blob(combined_ndjson_path)\n",
    "        combined_ndjson_blob.upload_from_string(combined_ndjson_data, content_type='application/json')\n",
    "\n",
    "        try:\n",
    "            load_into_bigquery_from_gcs(combined_ndjson_path)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading combined NDJSON into BigQuery: {e}\")\n",
    "            # Handle failure accordingly\n",
    "        else:\n",
    "            print(\"Data loaded into BigQuery successfully.\")\n",
    "    # === New Code End ===\n",
    "\n",
    "    # After processing all files, calculate total processing time and print summary\n",
    "    script_end_time = datetime.now(paris_tz)\n",
    "    total_duration = script_end_time - script_start_time\n",
    "    total_duration_str = str(total_duration)\n",
    "\n",
    "    # Print the summary\n",
    "    print(\"====================================\")\n",
    "    print(f\"Total number of files processed: {total_files_processed}\")\n",
    "    print(f\"Total processing time: {total_duration_str}\")\n",
    "    print(\"====================================\")\n",
    "\n",
    "    # --- SNIPPET 2. Start ---\n",
    "\n",
    "    current_files_count = len(files_to_process)\n",
    "    if run_number >= MAX_RUNS:\n",
    "        print(f\"Reached the maximum of {MAX_RUNS} runs. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # If it's not the first run, stop if zero files or same as previous\n",
    "    if run_number > 1:\n",
    "        if current_files_count == 0 or current_files_count == previous_files_count:\n",
    "            print(\"No new files to process or same as previous run. Stopping.\")\n",
    "            break\n",
    "\n",
    "    previous_files_count = current_files_count\n",
    "\n",
    "    # --- SNIPPET 2. End ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pf0BLNxe4jRb",
   "metadata": {
    "id": "pf0BLNxe4jRb"
   },
   "source": [
    "## BigQuery Upload Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3_VqQKyf9o",
   "metadata": {
    "id": "fe3_VqQKyf9o"
   },
   "source": [
    "This pipeline updates `FILE_LIST`, a BigQuery table that tracks all the files. It records files that have been parsed and successfully added to the `CHUNKS` table in BigQuery, indicating their availability for RAG.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZ2cjdxbOpLu",
   "metadata": {
    "id": "NZ2cjdxbOpLu"
   },
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define the UPDATE query with an additional condition\n",
    "UPDATE_QUERY = \"\"\"\n",
    "UPDATE `your-project-id.YOUR_DATASET.FILE_LIST` AS T\n",
    "SET\n",
    "    T.added_to_bq = TRUE,\n",
    "    T.timestamp_added = U.ag_chunks_session_end\n",
    "FROM (\n",
    "    SELECT\n",
    "        FL.file_number,\n",
    "        UC.j_file_number,\n",
    "        UC.ag_chunks_session_end\n",
    "    FROM `your-project-id.YOUR_DATASET.FILE_LIST` AS FL\n",
    "    LEFT JOIN (\n",
    "        SELECT\n",
    "            j_file_number,\n",
    "            ANY_VALUE(ag_chunks_session_end) AS ag_chunks_session_end\n",
    "        FROM `your-project-id.YOUR_DATASET.CHUNKS`\n",
    "        GROUP BY j_file_number\n",
    "    ) AS UC\n",
    "    ON FL.file_number = UC.j_file_number\n",
    ") AS U\n",
    "WHERE\n",
    "    T.file_number = U.file_number\n",
    "    AND U.j_file_number IS NOT NULL  -- Ensure there is a match in CHUNKS\n",
    "    AND T.added_to_bq = FALSE;  -- Update only rows where added_to_bq is currently FALSE\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "try:\n",
    "    query_job = client.query(UPDATE_QUERY)\n",
    "    query_job.result()  # Wait for the query to finish\n",
    "\n",
    "    # Get the exact count of updated rows\n",
    "    updated_rows = query_job.num_dml_affected_rows\n",
    "    print(f\"Update query executed successfully. Rows updated: {updated_rows}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orh47FQp6NLF",
   "metadata": {
    "id": "orh47FQp6NLF"
   },
   "source": [
    "# Update RAG Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AzFLGl_by7YQ",
   "metadata": {
    "id": "AzFLGl_by7YQ"
   },
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nqwi-5ufWp_B",
   "metadata": {
    "id": "Nqwi-5ufWp_B"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"your-project-id\"  # @param {type:\"string\"}\n",
    "LOCATION = \"europe-west4\"  # @param {type:\"string\"}\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_68282UqlxrR",
   "metadata": {
    "id": "_68282UqlxrR"
   },
   "outputs": [],
   "source": [
    "DATASET = \"KNOWLEDGE_BASE\"  # @param {type:\"string\"}\n",
    "TABLE = \"CORPUS\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2tRYGd_Mv_ix",
   "metadata": {
    "id": "2tRYGd_Mv_ix"
   },
   "outputs": [],
   "source": [
    "embedding_model = VertexAIEmbeddings(\n",
    "    model_name=\"text-embedding-005\", project=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gLxHuLY1lxrS",
   "metadata": {
    "id": "gLxHuLY1lxrS"
   },
   "outputs": [],
   "source": [
    "bq_store = BigQueryVectorStore(\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    dataset_name=DATASET,\n",
    "    table_name=TABLE,\n",
    "    embedding=embedding_model,\n",
    "    distance_type=\"EUCLIDEAN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HNn58uO0fIiP",
   "metadata": {
    "id": "HNn58uO0fIiP"
   },
   "outputs": [],
   "source": [
    "# --- Setup your BigQuery client ---\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# --- References to dataset and table ---\n",
    "dataset_ref = bq_client.dataset(DATASET)\n",
    "table_ref = dataset_ref.table(TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gK4wqIdczCiR",
   "metadata": {
    "id": "gK4wqIdczCiR"
   },
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MGH1QRCmOrab",
   "metadata": {
    "id": "MGH1QRCmOrab"
   },
   "outputs": [],
   "source": [
    "# --- Define your QUERY ---\n",
    "QUERY_EXISTING = f\"\"\"\n",
    "SELECT\n",
    "  a_chunk_id,\n",
    "  b_page_number,\n",
    "  c_file_pages,\n",
    "  CAST(d_document_date AS TIMESTAMP) AS d_document_date,\n",
    "  e_chunk_type,\n",
    "  f_chunk_title,\n",
    "  CASE\n",
    "    WHEN e_chunk_type = 'table' THEN CONCAT(f_chunk_title, '. ', i_chunk_summary, ' ', g_chunk_contents)\n",
    "    ELSE g_chunk_contents\n",
    "  END AS g_chunk_contents,\n",
    "  ARRAY_TO_STRING(h_chunk_keywords, ',') AS h_chunk_keywords,\n",
    "  i_chunk_summary,\n",
    "  j_file_number,\n",
    "  k_file_hash,\n",
    "  l_file_name,\n",
    "  m_file_layout,\n",
    "  n_document_name,\n",
    "  o_document_author,\n",
    "  p_document_type,\n",
    "  ARRAY_TO_STRING(q_document_keywords, ',') AS q_document_keywords,\n",
    "  r_document_summary,\n",
    "  s_page_gsutil,\n",
    "  t_page_url,\n",
    "  u_file_gsutil,\n",
    "  v_file_url,\n",
    "  aa_document_session_id,\n",
    "  ae_chunks_session_id\n",
    "FROM `your-project-id.YOUR_DATASET.CHUNKS`\n",
    "WHERE\n",
    "  a_chunk_id NOT IN (\n",
    "    SELECT a_chunk_id\n",
    "    FROM `your-project-id.{DATASET}.{TABLE}`\n",
    "  )\n",
    "  AND (\n",
    "    (e_chunk_type = 'text' AND LENGTH(g_chunk_contents) >= 300)\n",
    "    OR\n",
    "    (e_chunk_type IN ('table', 'chart', 'diagram', 'image'))\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "# print(\"Main query is executed\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OAbhmQmQjzx3",
   "metadata": {
    "id": "OAbhmQmQjzx3"
   },
   "outputs": [],
   "source": [
    "QUERY_NEW = f\"\"\"\n",
    "SELECT\n",
    "  a_chunk_id,\n",
    "  b_page_number,\n",
    "  c_file_pages,\n",
    "  CAST(d_document_date AS TIMESTAMP) AS d_document_date,\n",
    "  e_chunk_type,\n",
    "  f_chunk_title,\n",
    "  CASE\n",
    "    WHEN e_chunk_type = 'table' THEN CONCAT(f_chunk_title, '. ', i_chunk_summary, ' ', g_chunk_contents)\n",
    "    ELSE g_chunk_contents\n",
    "  END AS g_chunk_contents,\n",
    "  ARRAY_TO_STRING(h_chunk_keywords, ',') AS h_chunk_keywords,\n",
    "  i_chunk_summary,\n",
    "  j_file_number,\n",
    "  k_file_hash,\n",
    "  l_file_name,\n",
    "  m_file_layout,\n",
    "  n_document_name,\n",
    "  o_document_author,\n",
    "  p_document_type,\n",
    "  ARRAY_TO_STRING(q_document_keywords, ',') AS q_document_keywords,\n",
    "  r_document_summary,\n",
    "  s_page_gsutil,\n",
    "  t_page_url,\n",
    "  u_file_gsutil,\n",
    "  v_file_url,\n",
    "  aa_document_session_id,\n",
    "  ae_chunks_session_id\n",
    "FROM `your-project-id.YOUR_DATASET.CHUNKS`\n",
    "WHERE\n",
    "  (\n",
    "    (e_chunk_type = 'text' AND LENGTH(g_chunk_contents) >= 300)\n",
    "    OR\n",
    "    (e_chunk_type IN ('table', 'chart', 'diagram', 'image'))\n",
    "  )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EbRfDMuQj8aP",
   "metadata": {
    "id": "EbRfDMuQj8aP"
   },
   "outputs": [],
   "source": [
    "# --- Decide which query to use based on table existence and schema ---\n",
    "try:\n",
    "    table = bq_client.get_table(table_ref)  # API call\n",
    "    # If the table exists, check if the schema is not empty\n",
    "    if table.schema:\n",
    "        print(f\"Table '{TABLE}' found with schema. Using QUERY_EXISTING.\")\n",
    "        chosen_query = QUERY_EXISTING\n",
    "    else:\n",
    "        print(f\"Table '{TABLE}' found but no schema. Using QUERY_NEW.\")\n",
    "        chosen_query = QUERY_NEW\n",
    "except NotFound:\n",
    "    print(f\"Table '{TABLE}' does not exist. Using QUERY_NEW.\")\n",
    "    chosen_query = QUERY_NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stGW801TzP6R",
   "metadata": {
    "id": "stGW801TzP6R"
   },
   "source": [
    "## Setup Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_GAgjoUUO9Os",
   "metadata": {
    "id": "_GAgjoUUO9Os"
   },
   "outputs": [],
   "source": [
    "# --- Continue with the loader ---\n",
    "loader = BigQueryLoader(\n",
    "    chosen_query,\n",
    "    page_content_columns=[\"g_chunk_contents\"],\n",
    "    metadata_columns=[\n",
    "        \"a_chunk_id\",\n",
    "        \"b_page_number\",\n",
    "        \"c_file_pages\",\n",
    "        \"d_document_date\",\n",
    "        \"e_chunk_type\",\n",
    "        \"f_chunk_title\",\n",
    "        \"h_chunk_keywords\",\n",
    "        \"i_chunk_summary\",\n",
    "        \"j_file_number\",\n",
    "        \"k_file_hash\",\n",
    "        \"l_file_name\",\n",
    "        \"m_file_layout\",\n",
    "        \"n_document_name\",\n",
    "        \"o_document_author\",\n",
    "        \"p_document_type\",\n",
    "        \"q_document_keywords\",\n",
    "        \"r_document_summary\",\n",
    "        \"s_page_gsutil\",\n",
    "        \"t_page_url\",\n",
    "        \"u_file_gsutil\",\n",
    "        \"v_file_url\",\n",
    "        \"aa_document_session_id\",\n",
    "        \"ae_chunks_session_id\"\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rExlHLjVO4dL",
   "metadata": {
    "id": "rExlHLjVO4dL"
   },
   "outputs": [],
   "source": [
    "# --- Optional: Run a separate COUNT query to see how many rows match ---\n",
    "count_query = f\"SELECT COUNT(*) as row_count FROM ({chosen_query})\"\n",
    "count_job = bq_client.query(count_query)\n",
    "count_result = list(count_job.result())\n",
    "num_rows = count_result[0].row_count if count_result else 0\n",
    "\n",
    "print(f\"Number of new rows meeting criteria: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JPXM4uxOc62f",
   "metadata": {
    "id": "JPXM4uxOc62f"
   },
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "num_docs = len(docs)\n",
    "\n",
    "# Post-process to remove the unwanted \"column name:\"\n",
    "for doc in docs:\n",
    "    doc.page_content = doc.page_content.replace(\"g_chunk_contents: \", \"\")\n",
    "\n",
    "print(\"Number of documents loaded into Python memory:\", num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5JAz9JVbzYGA",
   "metadata": {
    "id": "5JAz9JVbzYGA"
   },
   "source": [
    "## Run Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uF6ncnzdYUXV",
   "metadata": {
    "id": "uF6ncnzdYUXV"
   },
   "outputs": [],
   "source": [
    "# Only call add_documents if docs is non-empty\n",
    "if num_docs > 0:\n",
    "    doc_ids = bq_store.add_documents(docs)\n",
    "    print(\"Number of documents successfully added to vector store:\", len(doc_ids))\n",
    "    print(\"Document IDs:\", doc_ids)\n",
    "else:\n",
    "    print(\"No new rows found. Skipping vector store insertion.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nikolai.len (Feb 19, 2025, 4:18:43 PM)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
